{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9deccd18",
   "metadata": {},
   "source": [
    "In this notebook, the goal is to build a crawler that crawls the given seed url. <br>\n",
    "Data that are scrapped is stored in a database as <i>database.csv</i> in the same working directory.<br><br>\n",
    "seed_url = https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0c9511",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c6d3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import schedule\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1bb0a",
   "metadata": {},
   "source": [
    "# Check the robots.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d1e86c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Agent: *\n",
      "Crawl-Delay: 5\n",
      "Disallow: /*?*format=rss\n",
      "Disallow: /*?*export=xls\n",
      "Sitemap: https://pureportal.coventry.ac.uk/sitemap.xml\n"
     ]
    }
   ],
   "source": [
    "robots_text_page = requests.get(\"https://pureportal.coventry.ac.uk/en/robots.txt\")\n",
    "print(robots_text_page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85138fc",
   "metadata": {},
   "source": [
    "We need to delay the crawler for 5 seconds after each crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30529d2",
   "metadata": {},
   "source": [
    "# Create some helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4aa223",
   "metadata": {},
   "source": [
    "### Find total number of pages to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429097f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTotalPages():\n",
    "    \"\"\"\n",
    "    This function finds the total number of pages to crawl.\n",
    "    \"\"\"\n",
    "    total_research = soup.select_one(\".count\").get_text()  # gives string\n",
    "    total_research = int(''.join(filter(str.isdigit, total_research)))  # convert to int\n",
    "\n",
    "    total_pages = math.ceil(total_research / 50)  # since 50 results in one page\n",
    "    return total_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98f77a",
   "metadata": {},
   "source": [
    "###### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65f78963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_url = \"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning\"\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "findTotalPages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88dfc5",
   "metadata": {},
   "source": [
    "### Create list of urls to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac93e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageUrls():\n",
    "    \"\"\"\n",
    "    This function creates the list of urls to crawl.\n",
    "    \"\"\"\n",
    "    total_pages = findTotalPages()\n",
    "    urls = [(seed_url + \"/publications/?page=\" + str(page)) for page in range(0, total_pages)]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214182c",
   "metadata": {},
   "source": [
    "###### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620f9dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=0',\n",
       " 'https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=1',\n",
       " 'https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=2',\n",
       " 'https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=3',\n",
       " 'https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=4',\n",
       " 'https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/?page=5']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPageUrls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d912f6",
   "metadata": {},
   "source": [
    "### Create list of CGL profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "106e843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CGLprofiles():\n",
    "    \"\"\"\n",
    "    This function creates a list of profiles on CGL.\n",
    "    \"\"\"\n",
    "    profiles_link = seed_url + \"/persons\"\n",
    "    profile_page = requests.get(profiles_link)\n",
    "    soup3 = BeautifulSoup(profile_page.text, \"html.parser\")\n",
    "\n",
    "    persons_list = soup3.find_all(\"a\", class_=\"link person\")\n",
    "    profiles = []\n",
    "    for person in persons_list:\n",
    "        profiles.append(person.get_text())\n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157b966",
   "metadata": {},
   "source": [
    "###### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba98701",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sian Alsop',\n",
       " 'Dimitar Angelov',\n",
       " 'Rami Ayoubi',\n",
       " 'Ema Baukaite',\n",
       " 'Julia Carroll',\n",
       " 'Jacqueline Cawston',\n",
       " 'Megan Crawford',\n",
       " 'QueAnh Dang',\n",
       " 'Alun DeWinter',\n",
       " 'Ken Fero',\n",
       " 'Mark Hodds',\n",
       " 'Sylwia Holmes',\n",
       " 'Elizabeth Horton',\n",
       " 'Jaya Jacobo',\n",
       " 'Emmanuel Johnson',\n",
       " 'Mehmet Karakus',\n",
       " 'Luca Morini',\n",
       " 'Marina Orsini-Jones',\n",
       " 'Charlotte Price',\n",
       " 'Steve Raven',\n",
       " 'Carlo Tramontano',\n",
       " 'Katherine Wimpenny']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles = CGLprofiles()\n",
    "profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef872f53",
   "metadata": {},
   "source": [
    "### Create or update the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fcdd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateDB():\n",
    "    \"\"\"\n",
    "    This function updates the database.\n",
    "    If no database exists, it creates one.\n",
    "    If database already exists, it removes the duplicates.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile('database.csv'):\n",
    "        database = pd.DataFrame(columns=['Title', 'Authors', 'Date', 'Publication_Link'])\n",
    "        database.to_csv('database.csv', index=False)\n",
    "    else:\n",
    "        database = pd.read_csv(\"database.csv\")\n",
    "        database = database.drop_duplicates()\n",
    "        database.to_csv('database.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6f084",
   "metadata": {},
   "source": [
    "# Crawler Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78fa66f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler():\n",
    "    \"\"\"\n",
    "    This is the main crawler.\n",
    "    It crawls the seed url and stores the Title, Authors, Date and Publication Link\n",
    "    in a database.\n",
    "    It checks if the author is from CGL, and only stores the data if at least one of\n",
    "    the authors has profile on CGL.\n",
    "    It also counts the total number of publications crawled, total CGL publications,\n",
    "    and non CGL publications.\n",
    "    \"\"\"\n",
    "    url_list = getPageUrls()\n",
    "    updateDB()\n",
    "\n",
    "    count = 0\n",
    "    count_cgl = 0\n",
    "    count_non_cgl = 0\n",
    "\n",
    "    while (url_list != []):\n",
    "        url = url_list.pop(0)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        publications = soup.find_all('div', class_='result-container')\n",
    "\n",
    "        for publication in publications:\n",
    "            title = publication.select_one('.result-container .title').get_text()\n",
    "\n",
    "            link = publication.select_one('.result-container .title .link').get('href')\n",
    "\n",
    "            date = publication.select_one('.date').get_text()\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "            paper = requests.get(link)\n",
    "            soup2 = BeautifulSoup(paper.text, \"html.parser\")\n",
    "            authors = soup2.select_one(\"p\", class_=\"relations persons\")\n",
    "            authors = authors.get_text()\n",
    "            authors = re.sub(r'\\s*\\([^)]*\\)', '', authors).split(\", \")  # list of authors\n",
    "            \n",
    "            # check if any author is from CGL\n",
    "            if any(author in authors for author in profiles):\n",
    "                count_cgl += 1\n",
    "\n",
    "                db = pd.read_csv('database.csv')\n",
    "                new_data = pd.DataFrame({'Title': [title],\n",
    "                                         'Authors': [authors],\n",
    "                                         'Date': [date],\n",
    "                                         'Publication_Link': [link]})\n",
    "                db = pd.concat([db, new_data], ignore_index=True)\n",
    "                db.to_csv('database.csv', index=False)\n",
    "\n",
    "            else:\n",
    "                count_non_cgl += 1\n",
    "            count += 1\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "    updateDB()\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print('Last crawl at: ', current_time)\n",
    "    \n",
    "    print('total publications crawled:', count)\n",
    "    print('cgl publications:', count_cgl)\n",
    "    print('non-cgl publications:', count_non_cgl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d185d",
   "metadata": {},
   "source": [
    "# Run and Schedule the crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5fc2d",
   "metadata": {},
   "source": [
    "Please note that the below code runs indefinitely until it is interrupted.<br>\n",
    "It is scheduled to crawl the given url on every Monday at 12 o'clock midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650975a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last crawl at:  2023-07-27 23:03:11\n",
      "total publications crawled: 286\n",
      "cgl publications: 195\n",
      "non-cgl publications: 91\n"
     ]
    }
   ],
   "source": [
    "seed_url = \"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning\"\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "time.sleep(5)\n",
    "profiles = CGLprofiles()\n",
    "\n",
    "# start crawling\n",
    "crawler()\n",
    "\n",
    "# schedule the crawler to run every monday at 12:00 AM\n",
    "schedule.every().monday.at(\"00:00\").do(crawler)\n",
    "\n",
    "# run the scheduler continuously\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "    # time.sleep(1) function pauses the program for 1 second between iterations to avoid unnecessary CPU usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c1a08",
   "metadata": {},
   "source": [
    "Remember to terminate the program manually when you want to stop the crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba22e2a",
   "metadata": {},
   "source": [
    "Please also check the <i>indexing_and_query_processing.ipynb</i> after you crawl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f7cae",
   "metadata": {},
   "source": [
    "<i>Thank you for reading my notebook.\n",
    "<br>\n",
    "<br>Avishek K C<br>\n",
    "2023</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
